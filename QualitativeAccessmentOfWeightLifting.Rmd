---
title: "Qualitative accessment of weight lifting exercise"
author: "Mia Eden"
date: "21 February 2018"
output: 
  html_document:
    keep_md: yes
  md_document:
    variant: markdown_github
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
options(width=120)

library(knitr); library(printr); library(dplyr); library(rattle); library(kableExtra)
library(caret); library(ggplot2); library(randomForest)
```

### Executive Summary
People regularly quantify how much of a particular physical activity they do, Using devices such as Jawbone Up, Nike FuelBand, and Fitbit. But rarely quantify how well they do it. 
In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants whom were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Our goal is to predict the manner in which they did the exercise. This is the *classe* variable in the training set: 

*   A) performed correctly according to specification   
*   B) incorrectly throwing elbow to front  
*   C) incorrectly lifting the dumbbell only halfway  
*   D) incorrectly lowering the dumbbell only halfway   
*   E) incorrectly throwing the hips to the front  

For more information about dataset please visit [HAR website](http://groupware.les.inf.puc-rio.br/har) - section on the Weight Lifting Exercise Dataset.  

*   [Training data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)   
*   [Test data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)  

### Data processing

**Read HAR data** for train and test data from working directory, folder "data".  

```{r read}
# read train and test data, replacing both division error strings #DIV/0! and blank fields with NA values
trainData <- read.csv("./data/pml-training.csv",head=TRUE,sep=",",na.strings=c("NA","#DIV/0!",""))  
testData <- read.csv("./data/pml-testing.csv", head=TRUE, sep=",",na.strings=c("NA","#DIV/0!",""))    
```

Check **dimension** of datasets.

```{r dim}
data.frame("dataset" = c("train","test"), "numberOfRows" = c(dim(trainData)[1], dim(testData)[1]),
    "numberOfColumns" = c(dim(trainData)[2], dim(testData)[2])) %>%
    kable() %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```

Verify if there are **missing values** in the dataset columns and **percentage** to total, since the prediction algorithm will fail to work properly with missing values. Find the columns that have more than **95%** missing values, as applicable.

```{r findHighNa}
naTrain <- sapply(trainData, function(x) {sum(is.na(x)==TRUE)/length(x)})
naTest <- sapply(testData, function(x) {sum(is.na(x)==TRUE)/length(x)})
data.frame ("trainColNa95Perc" = sum(naTrain > .95), "testColNa95Perc"= sum(naTest > .95)) %>%
    kable() %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

Remove those **100 columns that have over 95% NA** (missing values).

```{r removeNa}
trainData2 <- trainData[, names(which(naTrain<=.95))]
testData2 <- testData[, names(which(naTest<=.95))]
```

Remove **zero covariates** (predictors that do not contribute much in prediction, for having very little variability in them).  

```{r near0}
nsvTrain <- nearZeroVar(trainData2,saveMetrics=TRUE)
nsvTest <- nearZeroVar(testData2,saveMetrics=TRUE)
trainData3 <- trainData2[,which(nsvTrain$nzv==FALSE)]
testData3 <- testData2[,which(nsvTest$nzv==FALSE)]
```

Verify if **any other variable** should also be removed.  

```{r checkColumns, eval = FALSE}
str(trainData3); str(testData3)
```

Choose to only keep predictors related to the **basic original measurement of movement** on belt, glove, arm-band or dumbell. Will not include in the model variables related to unique identifiers, time series, or calculations such as total.

```{r badVariables}
trainData4 <- trainData3[, which(grepl("_x|_y|_z|^yaw|^pitch|^roll|classe", names(trainData3)))]
testData4 <- testData3[, which(grepl("_x|_y|_z|^yaw|^pitch|^roll|classe", names(testData3)))]
```

### Building the model

Take a look at **"classe"** levels (this variable predicts the manner in which exercise was performed) to confirm values.  

```{r classe}
table(trainData4$classe) %>%
    kable() %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

**Split** the train data into 70% training and 30% validation test on column class for building/ validating the model.

```{r dataPartition}
set.seed(24642)
inTrain <- createDataPartition (trainData4$classe, p=0.7, list=FALSE)
training <- trainData4 [inTrain ,]
validating <- trainData4 [- inTrain,]
```

Since this is a **classification** prediction type and there is a **large number of variables** as predictors, we chose the **random forest** method for modeling. Random Forest grows many trees (or a forest), through bootstrap sampling (sampling with replacement). At each node, a subset of predictors m, is selected at random out of M total predictors. The best split on these m is used to split the node. And m is held constant as the forest grows. Also random forest handles well outliers and correlated variables.  

```{r buildModel, cache=TRUE}
# Random forest with 10 folds cross validation and 150 trees
rFFit <- train(classe ~ ., data = training, method = "rf",  
    trControl = trainControl(method = "cv", number=10),
             allowParallel=TRUE, ntree=150)
print(rFFit)
```

**Cross-validation** was performed inside the model so a separate test set to obtain the unbiased estimate of the test set error is not really necessary. And the **error rate** is expected to be low, due to model selected.

### Visualizing the model

```{r modelPlot}
plot(rFFit, col = "turquoise3", lwd = 2, main = "Model Accuracy versus nbr. of Predictors")
plot(rFFit$finalModel, main = "Final Model Error versus nbr. of Trees")
plot(varImp(rFFit), col = "turquoise3", main = "Variables by Importance")

```

Notice on above charts:   

*   The highest accuracy was obtained with only 2 randomly selected predictors  
*   50 trees would be sufficient for this study  
*   Top 2 most important variables are: roll-belt and yaw-belt  

### Validating the model

Evaluate the fitted model, using it to predict classe in validation data set. Compute the confusion matrix and associated statistics to asses the preformance of the model fit.  

```{r confusion}
predictrFFit <- predict(rFFit, newdata = validating)
confusionMatrix(data = predictrFFit, validating$classe)
```

The **Accuracy** of our model seems very good, at **99.3%**.  Predictions look reliable.  
We now plot top 2 variables for a simple visualization of prediction.  

```{r plotTop2}
correctPrediction <- predictrFFit == validating$classe
qplot(roll_belt, yaw_belt, data=validating, colour=correctPrediction, main = "Yaw Belt versus Roll Belt and Correct Prediction") 

```

### Applying final model to test dataset

```{r testModel}
finalPrediction <- predict(rFFit, testData4)
data.frame("case nbr"=c(1:20),"class prediction" = finalPrediction) %>%
    kable() %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```

### Conclusions

Considering the high accuracy obtained from our model, it appears very reasonable to predict the manner in which people exercise. Sensitivity of classe A prediction is 100%. Using wearable devices or sensors to collect data and providing real-time feedback to users on how well they are performing the exercise would highly enhance work-out technique. 
